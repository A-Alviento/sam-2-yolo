{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin\n",
    "Select kernel `segment-anything`\n",
    "\n",
    "Run this cell to import necessary packages and initialise SAM model and mask generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for operations on masks \n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting images\n",
    "import cv2 # for image processing\n",
    "from scipy import ndimage # for image processing\n",
    "import base64 # for encoding images\n",
    "\n",
    "import os # for file operations\n",
    "import shutil # for file operations\n",
    "import glob # for file operations\n",
    "import pickle # for data serialization\n",
    "import json # for reading json files\n",
    "\n",
    "import torch # for deep learning\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor # for the SAM model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset\n",
    "from IPython.display import clear_output # for clearing the output\n",
    "import yaml # for creating yaml file for YOLO dataset\n",
    "\n",
    "from ultralytics import YOLO # for YOLO model\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget # for creating bounding box widget\n",
    "import ipywidgets as widgets # for creating widgets\n",
    "\n",
    "from bboxidea_functions import * # for functions used in the notebook\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1' # since mps does not support all the operations, we need to enable fallback to cpu for some operations\n",
    "\n",
    "sam_checkpoint = \"../models/sam_vit_b_01ec64.pth\" # Path to the checkpoint file\n",
    "model_type = 'vit_b' # Model type\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" # Use GPU if available, otherwise use CPU\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) # Load the model\n",
    "sam.to(device=device) # Move the model to the device\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam) # Create a mask generator\n",
    "mask_predictor = SamPredictor(sam) # Create a mask predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m frame_interval \u001b[39m=\u001b[39m \u001b[39m900\u001b[39m \u001b[39m# specify the frame interval to extract from the video\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# extract frames from videos in 1-source folder, extract them to 2-source-extracted folder, and split them into train and valid sets in 3-dataset folder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m load_images_from_video(img_path, vid_path, ds_path, frame_interval)\n",
      "File \u001b[0;32m~/Desktop/cs-projects/sam-2-yolo/notebooks/bboxidea_functions.py:203\u001b[0m, in \u001b[0;36mload_images_from_video\u001b[0;34m(img_path, vid_path, ds_path, frame_interval)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_images_from_video\u001b[39m(img_path, vid_path, ds_path, frame_interval):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(vid_path):\n\u001b[1;32m    204\u001b[0m         extract_frames(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(vid_path, filename), img_path, frame_interval)\n\u001b[1;32m    206\u001b[0m     split_data(img_path, ds_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "vid_path = '../1-source/'\n",
    "img_path = '../2-source-extracted/'\n",
    "ds_path = '../3-dataset/'\n",
    "frame_interval = 900 # specify the frame interval to extract from the video\n",
    "\n",
    "# extract frames from videos in 1-source folder, extract them to 2-source-extracted folder, and split them into train and valid sets in 3-dataset folder\n",
    "load_images_from_video(img_path, vid_path, ds_path, frame_interval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/train/images/' # path to training images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "cur_img_idx = 0 # current image index\n",
    "classes = ['tiger'] # list of classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n"
     ]
    }
   ],
   "source": [
    "# initialise the bbox widget\n",
    "w_bbox = BBoxWidget(\n",
    "    image = encode_image(os.path.join(path, images[cur_img_idx])),\n",
    "    classes=classes\n",
    ")\n",
    "# a progress bar to show how far we got\n",
    "w_progress = widgets.IntProgress(value=0, max=len(images), description='Progress')\n",
    "\n",
    "# initialise the buttons\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "button_prev = widgets.Button(description=\"Previous\")\n",
    "# combine the buttons and the bbox widget into a container\n",
    "w_container = widgets.VBox([\n",
    "    w_progress,\n",
    "    button_prev,\n",
    "    button_next,\n",
    "    w_bbox\n",
    "\n",
    "])\n",
    "\n",
    "# function that updates the image when the buttons are clicked so that the next or previous image is shown\n",
    "def update_image(change):\n",
    "    global cur_img_idx\n",
    "    annotations[images[cur_img_idx]] = w_bbox.bboxes # save the annotations for the current image before moving to the next image\n",
    "\n",
    "    if images[cur_img_idx] not in data: # this is for the first time the image is shown\n",
    "        data[images[cur_img_idx]] = []\n",
    "    \n",
    "    # move the current image index forward or backward\n",
    "    if change.description == \"Next\":\n",
    "        cur_img_idx = (cur_img_idx + 1) % len(images)\n",
    "    elif change.description == \"Previous\":\n",
    "        cur_img_idx = (cur_img_idx - 1) % len(images)\n",
    "    w_progress.value = cur_img_idx # update the progress bar\n",
    "    if images[cur_img_idx] not in data: # if the next image is not in the data dictionary, add it as an empty list\n",
    "        data[images[cur_img_idx]] = []\n",
    "\n",
    "    # check if annotations[cur_img_idx] exists\n",
    "    if images[cur_img_idx] in annotations:\n",
    "        w_bbox.bboxes = annotations[images[cur_img_idx]] # if it exists, load the annotations for the image\n",
    "    else:\n",
    "        w_bbox.bboxes = [] # if it doesn't exist, the image has no annotations, so set the bounding boxes to an empty list\n",
    "\n",
    "    w_bbox.image = encode_image_existing_mask(os.path.join(path, images[cur_img_idx]), data[images[cur_img_idx]])\n",
    "# add the update_image function to the buttons\n",
    "button_next.on_click(update_image)\n",
    "button_prev.on_click(update_image)\n",
    "\n",
    "# defines what happens when the submit button is clicked, which is to run SAM with the bounding boxes specified by the user and to display the result and save the result to the data dictionary for conversion to YOLO format later\n",
    "@w_bbox.on_submit\n",
    "def submit():\n",
    "    data[images[cur_img_idx]] = []\n",
    "\n",
    "    if len(w_bbox.bboxes) > 0:\n",
    "        w_bbox.image, poly_coords_list, h, w = encode_image_mask(os.path.join(path, images[cur_img_idx]), w_bbox.bboxes)\n",
    "        i = 0\n",
    "        for polygon_coords in poly_coords_list:\n",
    "            label_id = [classes.index(w_bbox.bboxes[i]['label'])]\n",
    "            flat_segment_coords = numpy_to_list(polygon_coords)\n",
    "\n",
    "            for j in range(len(flat_segment_coords)): # normalise the coordinates of the segment\n",
    "                if j%2 == 0:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/w\n",
    "                else:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/h\n",
    "            \n",
    "            data[images[cur_img_idx]].append(label_id + flat_segment_coords)\n",
    "            i += 1\n",
    "    else:\n",
    "        w_bbox.image = encode_image(os.path.join(path, images[cur_img_idx]))\n",
    "        data[images[cur_img_idx]] = [] # if no bounding boxes are specified, then the image is not annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7231bc98fd4b9baaf22baa650bed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(BBoxWidget(classes=['tiger'], colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#…"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_output()\n",
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/train/labels/') # output the data to txt files for YOLO training\n",
    "delete_empty_labels_and_images('../3-dataset/train/labels/', '../3-dataset/train/images/') # delete empty labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/valid/images/' # path to validation images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "cur_img_idx = 0 # current image index\n",
    "classes = ['tiger'] # list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0297e18316af451ca34f91401b57add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(BBoxWidget(classes=['tiger'], colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#…"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_output()\n",
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/valid/labels/') # output the data to txt files for YOLO training\n",
    "delete_empty_labels_and_images('../3-dataset/valid/labels/', '../3-dataset/valid/images/') # delete empty labels and"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files('../3-dataset/train/images/', '../3-dataset/train/labels/', '../dataset/train/images/', '../dataset/train/labels/')\n",
    "move_files('../3-dataset/valid/images/', '../3-dataset/valid/labels/', '../dataset/valid/images/', '../dataset/valid/labels/') # moves the data collected in 3-dataset to the actual dataset folder for use in YOLO training\n",
    "move_source_vid('../1-source/', '../dataset/source/') # moves the videos from 1-source to dataset/sources for documentation/backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(input(\"Are you sure you want to clear the source videos, extracted images, and dataset? (y/n): \")) == 'y':\n",
    "    clear_directory('../1-source/')\n",
    "    clear_directory('../2-source-extracted/')\n",
    "    clear_directory('../3-dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspath_ds = os.path.abspath('../dataset/') # get the absolute path of the dataset folder\n",
    "output_path = '../dataset/data.yaml' # path to the data.yaml file\n",
    "\n",
    "create_yaml(classes, abspath_ds, output_path) # create the data.yaml file in the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.117 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.114 🚀 Python-3.8.0 torch-2.0.1 MPS\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=../models/yolov8n-seg.pt, data=../dataset/data.yaml, epochs=10, patience=50, batch=3, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adrian/Desktop/cs-projects/sam-2-yolo/dataset/train/labels.cache... 13 images, 0 backgrounds, 0 corrupt: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adrian/Desktop/cs-projects/sam-2-yolo/dataset/valid/labels.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4/4 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0004921875), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 3 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10         0G        nan        nan        nan        nan          4        640: 100%|██████████| 5/5 [00:41<00:00,  8.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.43s/it]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10         0G        nan        nan        nan        nan          4        640: 100%|██████████| 5/5 [00:04<00:00,  1.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10         0G        nan        nan        nan        nan          1        640: 100%|██████████| 5/5 [00:07<00:00,  1.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10         0G        nan        nan        nan        nan          3        640: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10         0G        nan        nan        nan        nan          2        640: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10         0G        nan        nan        nan        nan          3        640: 100%|██████████| 5/5 [00:04<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10         0G        nan        nan        nan        nan          2        640: 100%|██████████| 5/5 [00:02<00:00,  1.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10         0G        nan        nan        nan        nan          4        640: 100%|██████████| 5/5 [00:03<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10         0G        nan        nan        nan        nan          2        640: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10         0G        nan        nan        nan        nan          2        640: 100%|██████████| 5/5 [00:04<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "10 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs/segment/train2/weights/last.pt, 6.7MB\n",
      "Optimizer stripped from runs/segment/train2/weights/best.pt, 6.7MB\n",
      "\n",
      "Validating runs/segment/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.114 🚀 Python-3.8.0 torch-2.0.1 MPS\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "Speed: 56.6ms preprocess, 383.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# YOLO Training\n",
    "model = YOLO('../models/yolov8n-seg.pt')\n",
    "model.train(data='../dataset/data.yaml', epochs=10, imgsz=640, batch=3, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    WARNING ⚠️ stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39m../models/yolov8n-seg.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m results \u001b[39m=\u001b[39m trained_model\u001b[39m.\u001b[39;49mpredict(source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m, show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 240\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/autobackend.py:314\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    311\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/modules/head.py:90\u001b[0m, in \u001b[0;36mSegment.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     89\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproto(x[\u001b[39m0\u001b[39;49m])  \u001b[39m# mask protos\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     bs \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]  \u001b[39m# batch size\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     mc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv4[i](x[i])\u001b[39m.\u001b[39mview(bs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnm, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl)], \u001b[39m2\u001b[39m)  \u001b[39m# mask coefficients\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:50\u001b[0m, in \u001b[0;36mProto.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Performs a forward pass through layers using an upsampled input image.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv3(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupsample(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x))))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = YOLO('../models/yolov8n-seg.pt')\n",
    "results = trained_model.predict(source='../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4', show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
