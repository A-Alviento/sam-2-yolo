{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin\n",
    "Select kernel `segment-anything`\n",
    "\n",
    "Run this cell to import necessary packages and initialise SAM model and mask generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import yaml\n",
    "import glob\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1' # since mps does not support all the operations, we need to enable fallback to cpu for some operations\n",
    "\n",
    "sam_checkpoint = \"../models/sam_vit_b_01ec64.pth\" # Path to the checkpoint file\n",
    "model_type = 'vit_b' # Model type\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" # Use GPU if available, otherwise use CPU\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) # Load the model\n",
    "sam.to(device=device) # Move the model to the device\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam) # Create a mask generator\n",
    "mask_predictor = SamPredictor(sam) # Create a mask predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the area of a mask (number of pixels)\n",
    "def get_area(mask):\n",
    "    area = 0\n",
    "    for row in mask:\n",
    "        for col in row:\n",
    "            if col:\n",
    "                area += 1\n",
    "    return area\n",
    "\n",
    "\n",
    "# this function returns the index of the mask with the largest area\n",
    "def get_max_area(masks):\n",
    "    max_area = 0\n",
    "    idx = 0\n",
    "    for i in range(len(masks)):\n",
    "        if(get_area(masks[i]) > max_area):\n",
    "            max_area = get_area(masks[i])\n",
    "            idx = i\n",
    "    return idx\n",
    "\n",
    "\n",
    "# this function draws the mask on the image\n",
    "def overlay_mask_on_image(image, coord):\n",
    "    # Ensure the mask is in 8-bit format\n",
    "    image = cv2.drawContours(image, coord, -1, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "# process the mask to remove the holes in the mask and return the largest region\n",
    "def process_mask(mask):\n",
    "    # Identify each separate region in the mask.\n",
    "    labeled_mask, num_labels = ndimage.label(mask)\n",
    "    \n",
    "    # Count the size of each region.\n",
    "    region_sizes = np.bincount(labeled_mask.flatten())\n",
    "    \n",
    "    # The first region (index 0) is the background, which we don't want to consider.\n",
    "    region_sizes[0] = 0\n",
    "    \n",
    "    # Find the largest region.\n",
    "    largest_region = np.argmax(region_sizes)\n",
    "    \n",
    "    # Create a mask that only includes the largest region.\n",
    "    largest_mask = (labeled_mask == largest_region)\n",
    "    \n",
    "    # Fill in the holes in this region.\n",
    "    filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
    "    \n",
    "    return filled_mask\n",
    "\n",
    "\n",
    "\n",
    "# extract the coordinates of the segment from SAM and store them in a list\n",
    "def extract_segment(mask):\n",
    "    binary_mask = np.array(mask) # get the segmentation of the mask and convert it to a numpy array\n",
    "    binary_mask = (binary_mask * 255).astype(np.uint8) # convert the mask to a binary mask\n",
    "\n",
    "    binary_mask = (process_mask(binary_mask) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours from the binary image\n",
    "    polygon_coords = [] # stores the coordinates of the vertices of the polygon\n",
    "    \n",
    "    for contour in contours:\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True) # approximate contour with accuracy proportional to the contour perimeter\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True) # approximate contour with the Douglas-Peucker algorithm\n",
    "\n",
    "        polygon_coords.append(approx) # add the coordinates of the vertices of the polygon to the list\n",
    "    return polygon_coords\n",
    "\n",
    "\n",
    "# this function is to encode the image for rendering by the widget\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded\n",
    "\n",
    "\n",
    "# this function is to encode the image with mask for rendering by the widget, whilst also returning the polygon coordinates of the mask and the original height and width of the image\n",
    "def encode_image_mask(filepath, boxes):\n",
    "    # read in the image file\n",
    "    image = cv2.imread(filepath)\n",
    "    h, w = image.shape[:2]\n",
    "    poly_coords_list = []\n",
    "    for box in boxes:\n",
    "        # convert the bbox to format expected by mask_predictor\n",
    "        box = np.array([\n",
    "            box['x'],\n",
    "            box['y'],\n",
    "            box['x'] + box['width'],\n",
    "            box['y'] + box['height']\n",
    "        ])\n",
    "\n",
    "        mask_predictor.set_image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        # predict the masks\n",
    "        masks, scores, logits = mask_predictor.predict(\n",
    "            box = box,\n",
    "            multimask_output = True\n",
    "        )\n",
    "\n",
    "        # get the index of the mask with the largest area\n",
    "        idx = get_max_area(masks)\n",
    "        mask = masks[idx]\n",
    "\n",
    "        # convert the pixel array format of the mask to a polygon coordinates\n",
    "        polygon_coords = extract_segment(mask)\n",
    "        poly_coords_list.append(polygon_coords)\n",
    "        # overlay the mask on the image\n",
    "        image = overlay_mask_on_image(image, polygon_coords)\n",
    "\n",
    "    # convert the image with mask back to bytes\n",
    "    is_success, im_buf_arr = cv2.imencode(\".jpg\", image)\n",
    "    byte_im = im_buf_arr.tobytes()\n",
    "\n",
    "    # encode to Base64 for rendering on the web\n",
    "    encoded = base64.b64encode(byte_im).decode('utf-8')\n",
    "    \n",
    "    return \"data:image/jpg;base64,\"+encoded, poly_coords_list, h, w\n",
    "\n",
    "\n",
    "\n",
    "# converts the list of numpy array to a list a list for easier manipulation\n",
    "def numpy_to_list(numpy_arr):\n",
    "    coord_list = [coord[0].tolist() for array in numpy_arr for coord in array] # Convert each numpy array in the list to a regular list and extract the inner lists into coord_list\n",
    "    flat_list = [] # stores the flattened list of coordinates\n",
    "    for coord in coord_list: # convert the list of lists into a flat list\n",
    "        flat_list.append(coord[0]) # append the x coordinate\n",
    "        flat_list.append(coord[1]) # append the y coordinate\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "# this is used to extract the frames from a video file and output into specified directory as jpg images\n",
    "def extract_frames(video_path, output_dir, frame_interval=300):\n",
    "    filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"Could not open video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if fps >= 50:\n",
    "        frame_interval *= 2\n",
    "    \n",
    "    frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = video.read()\n",
    "        if not success: \n",
    "            break\n",
    "\n",
    "        if frame_index % frame_interval == 0:\n",
    "            output_path = os.path.join(output_dir, f\"{filename}_frame_{frame_index}.png\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "\n",
    "# this is used to split the images into training and validation sets into the dataset folder\n",
    "def split_data(src_directory, out_directory, test_size=0.2):\n",
    "    os.makedirs(out_directory, exist_ok=True) # create the dataset directory\n",
    "\n",
    "    os.makedirs(os.path.join(out_directory, 'train', 'images'), exist_ok=False) # create the train images directory\n",
    "    os.makedirs(os.path.join(out_directory, 'valid', 'images'), exist_ok=False) # create the valid images directory\n",
    "    os.makedirs(os.path.join(out_directory, 'train', 'labels'), exist_ok=False) # create the train labels directory\n",
    "    os.makedirs(os.path.join(out_directory, 'valid', 'labels'), exist_ok=False) # create the valid labels directory\n",
    "\n",
    "    all_files = os.listdir(src_directory) # get all the files in the source directory\n",
    "    train_files, valid_files = train_test_split(all_files, test_size=test_size, random_state=42) # split the files into training and validation sets\n",
    "\n",
    "    # Move files into the train and valid directories\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(src_directory, file_name), os.path.join(out_directory, 'train', 'images', file_name))\n",
    "    for file_name in valid_files:\n",
    "        shutil.copy(os.path.join(src_directory, file_name), os.path.join(out_directory, 'valid', 'images', file_name))\n",
    "\n",
    "\n",
    "# this is used to load the images from the specified directory and output the data in the format required for YOLO training\n",
    "def load_images_from_video(img_path, vid_path, ds_path, frame_interval):\n",
    "    for filename in os.listdir(vid_path):\n",
    "        extract_frames(os.path.join(vid_path, filename), img_path, frame_interval)\n",
    "    \n",
    "    split_data(img_path, ds_path) # splits into train, valid sets and moves into ds_path folder\n",
    "\n",
    "\n",
    "# this is used to format the YOLO data into appropriate txt files for use in YOLO training\n",
    "def output_to_txt(data_dict, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for image_name, content in data_dict.items():\n",
    "        # remove the file extension from the image file name\n",
    "        base_name = os.path.splitext(image_name)[0]\n",
    "        # create the output file name by adding .txt extension\n",
    "        output_file_name = base_name + '.txt'\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        \n",
    "        with open(output_file_path, 'w') as f:\n",
    "            for line in content:\n",
    "                line_str = [str(item) for item in line]  # Convert all items to strings\n",
    "                f.write(' '.join(line_str))  # Join all items in the line with ',' as separator\n",
    "                f.write('\\n')  # Write a new line after each line\n",
    "    \n",
    "\n",
    "\n",
    "# this is used to move the data from 3-dataset to dataset for use in YOLO training\n",
    "def move_files(src_img, src_label, dest_img, dest_label):\n",
    "    # Check if destination directories exist, if not, create them\n",
    "    os.makedirs(dest_img, exist_ok=True)\n",
    "    os.makedirs(dest_label, exist_ok=True)\n",
    "\n",
    "    all_images = os.listdir(src_img)\n",
    "    all_labels = os.listdir(src_label)\n",
    "    \n",
    "    for image in all_images:\n",
    "        shutil.copy(os.path.join(src_img, image), os.path.join(dest_img, image))\n",
    "\n",
    "    \n",
    "    for label in all_labels:\n",
    "        shutil.copy(os.path.join(src_label, label), os.path.join(dest_label, label))\n",
    "\n",
    "\n",
    "# this is used to move the videos from 1-source to dataset/sources\n",
    "def move_source_vid(src_vid, dest_vid):\n",
    "    # Check if destination directories exist, if not, create them\n",
    "    os.makedirs(dest_vid, exist_ok=True)\n",
    "\n",
    "    all_videos = os.listdir(src_vid)\n",
    "    \n",
    "    for video in all_videos:\n",
    "        shutil.copy(os.path.join(src_vid, video), os.path.join(dest_vid, video))\n",
    "\n",
    "\n",
    "# clears directory\n",
    "def clear_directory(directory):\n",
    "    # Be careful with this function! It deletes all files and subdirectories in the specified directory\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "\n",
    "\n",
    "# this is used to create the data.yaml (necessary for YOLO training) file in the dataset folder \n",
    "def create_yaml(labels, path, output_path, train_path=\"train/images\", val_path=\"valid/images\", ):\n",
    "    # copies labels with the last two elemens removed\n",
    "    my_dict = {i: labels[i] for i in range(len(labels))}\n",
    "\n",
    "    data = {\n",
    "        'names': my_dict,\n",
    "        'path': path,\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as outfile: # write the data to the yaml file\n",
    "        yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '../1-source/'\n",
    "img_path = '../2-source-extracted/'\n",
    "ds_path = '../3-dataset/'\n",
    "frame_interval = 900 # specify the frame interval to extract from the video\n",
    "\n",
    "# extract frames from videos in 1-source folder, extract them to 2-source-extracted folder, and split them into train and valid sets in 3-dataset folder\n",
    "load_images_from_video(img_path, vid_path, ds_path, frame_interval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/train/images/' # path to training images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "cur_img_idx = 0 # current image index\n",
    "classes = ['tiger'] # list of classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3783946754.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n"
     ]
    }
   ],
   "source": [
    "# initialise the bbox widget\n",
    "w_bbox = BBoxWidget(\n",
    "    image = encode_image(os.path.join(path, images[cur_img_idx])),\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# initialise the buttons\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "button_prev = widgets.Button(description=\"Previous\")\n",
    "# combine the buttons and the bbox widget into a container\n",
    "w_container = widgets.VBox([\n",
    "    w_bbox,\n",
    "    button_prev,\n",
    "    button_next\n",
    "])\n",
    "\n",
    "# function that updates the image when the buttons are clicked so that the next or previous image is shown\n",
    "def update_image(change):\n",
    "    global cur_img_idx\n",
    "    annotations[images[cur_img_idx]] = w_bbox.bboxes\n",
    "    if change.description == \"Next\":\n",
    "        cur_img_idx = (cur_img_idx + 1) % len(images)\n",
    "    elif change.description == \"Previous\":\n",
    "        cur_img_idx = (cur_img_idx - 1) % len(images)\n",
    "        \n",
    "    # check if annotations[cur_img_idx] exists\n",
    "    if images[cur_img_idx] in annotations:\n",
    "        w_bbox.bboxes = annotations[images[cur_img_idx]]\n",
    "    else:\n",
    "        w_bbox.bboxes = []\n",
    "    w_bbox.image = encode_image(os.path.join(path, images[cur_img_idx]))\n",
    "# add the update_image function to the buttons\n",
    "button_next.on_click(update_image)\n",
    "button_prev.on_click(update_image)\n",
    "\n",
    "# defines what happens when the submit button is clicked, which is to run SAM with the bounding boxes specified by the user and to display the result and save the result to the data dictionary for conversion to YOLO format later\n",
    "@w_bbox.on_submit\n",
    "def submit():\n",
    "    if len(w_bbox.bboxes) > 0:\n",
    "        w_bbox.image, poly_coords_list, h, w = encode_image_mask(os.path.join(path, images[cur_img_idx]), w_bbox.bboxes)\n",
    "        \n",
    "        data[images[cur_img_idx]] = []\n",
    "        \n",
    "        i = 0\n",
    "        for polygon_coords in poly_coords_list:\n",
    "            label_id = [classes.index(w_bbox.bboxes[i]['label'])]\n",
    "            flat_segment_coords = numpy_to_list(polygon_coords)\n",
    "\n",
    "            for j in range(len(flat_segment_coords)): # normalise the coordinates of the segment\n",
    "                if j%2 == 0:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/w\n",
    "                else:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/h\n",
    "            \n",
    "            data[images[cur_img_idx]].append(label_id + flat_segment_coords)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7231bc98fd4b9baaf22baa650bed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(BBoxWidget(classes=['tiger'], colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#â€¦"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_output()\n",
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/train/labels/') # output the data to txt files for YOLO training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/valid/images/' # path to validation images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "cur_img_idx = 0 # current image index\n",
    "classes = ['tiger'] # list of classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3877965588.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3877965588.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3877965588.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
      "/var/folders/ry/tqw2n8k96rq816klwhrl6r200000gn/T/ipykernel_53616/3877965588.py:47: DeprecationWarning: Please use `binary_fill_holes` from the `scipy.ndimage` namespace, the `scipy.ndimage.morphology` namespace is deprecated.\n",
      "  filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n"
     ]
    }
   ],
   "source": [
    "# initialise the bbox widget\n",
    "w_bbox = BBoxWidget(\n",
    "    image = encode_image(os.path.join(path, images[cur_img_idx])),\n",
    "    classes=classes\n",
    ")\n",
    "\n",
    "# initialise the buttons\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "button_prev = widgets.Button(description=\"Previous\")\n",
    "# combine the buttons and the bbox widget into a container\n",
    "w_container = widgets.VBox([\n",
    "    w_bbox,\n",
    "    button_prev,\n",
    "    button_next\n",
    "])\n",
    "\n",
    "# function that updates the image when the buttons are clicked so that the next or previous image is shown\n",
    "def update_image(change):\n",
    "    global cur_img_idx\n",
    "    annotations[images[cur_img_idx]] = w_bbox.bboxes\n",
    "    if change.description == \"Next\":\n",
    "        cur_img_idx = (cur_img_idx + 1) % len(images)\n",
    "    elif change.description == \"Previous\":\n",
    "        cur_img_idx = (cur_img_idx - 1) % len(images)\n",
    "        \n",
    "    # check if annotations[cur_img_idx] exists\n",
    "    if images[cur_img_idx] in annotations:\n",
    "        w_bbox.bboxes = annotations[images[cur_img_idx]]\n",
    "    else:\n",
    "        w_bbox.bboxes = []\n",
    "    w_bbox.image = encode_image(os.path.join(path, images[cur_img_idx]))\n",
    "# add the update_image function to the buttons\n",
    "button_next.on_click(update_image)\n",
    "button_prev.on_click(update_image)\n",
    "\n",
    "# defines what happens when the submit button is clicked, which is to run SAM with the bounding boxes specified by the user and to display the result and save the result to the data dictionary for conversion to YOLO format later\n",
    "@w_bbox.on_submit\n",
    "def submit():\n",
    "    if len(w_bbox.bboxes) > 0:\n",
    "        w_bbox.image, poly_coords_list, h, w = encode_image_mask(os.path.join(path, images[cur_img_idx]), w_bbox.bboxes)\n",
    "        \n",
    "        data[images[cur_img_idx]] = []\n",
    "        \n",
    "        i = 0\n",
    "        for polygon_coords in poly_coords_list:\n",
    "            label_id = [classes.index(w_bbox.bboxes[i]['label'])]\n",
    "            flat_segment_coords = numpy_to_list(polygon_coords)\n",
    "\n",
    "            for j in range(len(flat_segment_coords)): # normalise the coordinates of the segment\n",
    "                if j%2 == 0:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/w\n",
    "                else:\n",
    "                    flat_segment_coords[j] = flat_segment_coords[j]/h\n",
    "            \n",
    "            data[images[cur_img_idx]].append(label_id + flat_segment_coords)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0297e18316af451ca34f91401b57add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(BBoxWidget(classes=['tiger'], colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#â€¦"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_output()\n",
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/valid/labels/') # output the data to txt files for YOLO training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files('../3-dataset/train/images/', '../3-dataset/train/labels/', '../dataset/train/images/', '../dataset/train/labels/')\n",
    "move_files('../3-dataset/valid/images/', '../3-dataset/valid/labels/', '../dataset/valid/images/', '../dataset/valid/labels/') # moves the data collected in 3-dataset to the actual dataset folder for use in YOLO training\n",
    "move_source_vid('../1-source/', '../dataset/source/') # moves the videos from 1-source to dataset/sources for documentation/backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(input(\"Are you sure you want to clear the source videos, extracted images, and dataset? (y/n): \")) == 'y':\n",
    "    clear_directory('../1-source/')\n",
    "    clear_directory('../2-source-extracted/')\n",
    "    clear_directory('../3-dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspath_ds = os.path.abspath('../dataset/') # get the absolute path of the dataset folder\n",
    "output_path = '../dataset/data.yaml' # path to the data.yaml file\n",
    "\n",
    "create_yaml(classes, abspath_ds, output_path) # create the data.yaml file in the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.117 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.114 ðŸš€ Python-3.8.0 torch-2.0.1 MPS\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=segment, mode=train, model=../models/yolov8n-seg.pt, data=../dataset/data.yaml, epochs=10, patience=50, batch=3, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/segment/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3263811 parameters, 3263795 gradients\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/adrian/Desktop/cs-projects/sam-2-yolo/dataset/train/labels.cache... 13 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/adrian/Desktop/cs-projects/sam-2-yolo/dataset/valid/labels.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/segment/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0004921875), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 3 dataloader workers\n",
      "Logging results to \u001b[1mruns/segment/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10         0G        nan        nan        nan        nan          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:41<00:00,  8.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.43s/it]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10         0G        nan        nan        nan        nan          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.66it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10         0G        nan        nan        nan        nan          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:07<00:00,  1.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.48it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10         0G        nan        nan        nan        nan          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.26it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10         0G        nan        nan        nan        nan          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10         0G        nan        nan        nan        nan          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.04it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10         0G        nan        nan        nan        nan          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.15it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10         0G        nan        nan        nan        nan          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.98it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10         0G        nan        nan        nan        nan          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.92it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10         0G        nan        nan        nan        nan          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.05it/s]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "\n",
      "10 epochs completed in 0.030 hours.\n",
      "Optimizer stripped from runs/segment/train2/weights/last.pt, 6.7MB\n",
      "Optimizer stripped from runs/segment/train2/weights/best.pt, 6.7MB\n",
      "\n",
      "Validating runs/segment/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.114 ðŸš€ Python-3.8.0 torch-2.0.1 MPS\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3258259 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.78s/it]\n",
      "                   all          4          4          0          0          0          0          0          0          0          0\n",
      "Speed: 56.6ms preprocess, 383.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/segment/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# YOLO Training\n",
    "model = YOLO('../models/yolov8n-seg.pt')\n",
    "model.train(data='../dataset/data.yaml', epochs=10, imgsz=640, batch=3, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    WARNING âš ï¸ stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "video 1/1 (1/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 294.7ms\n",
      "video 1/1 (2/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 236.3ms\n",
      "video 1/1 (3/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 226.3ms\n",
      "video 1/1 (4/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 230.5ms\n",
      "video 1/1 (5/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 223.1ms\n",
      "video 1/1 (6/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 227.3ms\n",
      "video 1/1 (7/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 220.4ms\n",
      "video 1/1 (8/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 218.0ms\n",
      "video 1/1 (9/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 218.1ms\n",
      "video 1/1 (10/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 237.2ms\n",
      "video 1/1 (11/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 236.5ms\n",
      "video 1/1 (12/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 233.5ms\n",
      "video 1/1 (13/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 249.4ms\n",
      "video 1/1 (14/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 229.9ms\n",
      "video 1/1 (15/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 259.9ms\n",
      "video 1/1 (16/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 234.3ms\n",
      "video 1/1 (17/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 239.8ms\n",
      "video 1/1 (18/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 2 birds, 228.7ms\n",
      "video 1/1 (19/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 2 birds, 259.3ms\n",
      "video 1/1 (20/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 2 birds, 298.5ms\n",
      "video 1/1 (21/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 285.1ms\n",
      "video 1/1 (22/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 259.0ms\n",
      "video 1/1 (23/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 234.3ms\n",
      "video 1/1 (24/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 228.4ms\n",
      "video 1/1 (25/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 221.4ms\n",
      "video 1/1 (26/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 227.4ms\n",
      "video 1/1 (27/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 222.9ms\n",
      "video 1/1 (28/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 226.3ms\n",
      "video 1/1 (29/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bird, 221.7ms\n",
      "video 1/1 (30/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.3ms\n",
      "video 1/1 (31/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.0ms\n",
      "video 1/1 (32/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 218.3ms\n",
      "video 1/1 (33/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 218.7ms\n",
      "video 1/1 (34/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.4ms\n",
      "video 1/1 (35/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.9ms\n",
      "video 1/1 (36/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.8ms\n",
      "video 1/1 (37/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.4ms\n",
      "video 1/1 (38/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.6ms\n",
      "video 1/1 (39/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.2ms\n",
      "video 1/1 (40/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.5ms\n",
      "video 1/1 (41/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 232.0ms\n",
      "video 1/1 (42/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 234.7ms\n",
      "video 1/1 (43/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.6ms\n",
      "video 1/1 (44/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.7ms\n",
      "video 1/1 (45/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 219.2ms\n",
      "video 1/1 (46/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 219.7ms\n",
      "video 1/1 (47/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.8ms\n",
      "video 1/1 (48/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.8ms\n",
      "video 1/1 (49/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 228.2ms\n",
      "video 1/1 (50/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 226.3ms\n",
      "video 1/1 (51/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.0ms\n",
      "video 1/1 (52/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.0ms\n",
      "video 1/1 (53/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.8ms\n",
      "video 1/1 (54/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.2ms\n",
      "video 1/1 (55/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 230.2ms\n",
      "video 1/1 (56/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 226.4ms\n",
      "video 1/1 (57/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 229.9ms\n",
      "video 1/1 (58/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 219.7ms\n",
      "video 1/1 (59/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 218.4ms\n",
      "video 1/1 (60/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.4ms\n",
      "video 1/1 (61/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.0ms\n",
      "video 1/1 (62/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.3ms\n",
      "video 1/1 (63/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 217.6ms\n",
      "video 1/1 (64/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.9ms\n",
      "video 1/1 (65/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.4ms\n",
      "video 1/1 (66/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 239.8ms\n",
      "video 1/1 (67/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.5ms\n",
      "video 1/1 (68/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.4ms\n",
      "video 1/1 (69/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.2ms\n",
      "video 1/1 (70/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.7ms\n",
      "video 1/1 (71/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.6ms\n",
      "video 1/1 (72/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.0ms\n",
      "video 1/1 (73/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.7ms\n",
      "video 1/1 (74/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.4ms\n",
      "video 1/1 (75/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 219.3ms\n",
      "video 1/1 (76/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.0ms\n",
      "video 1/1 (77/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.6ms\n",
      "video 1/1 (78/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bear, 224.2ms\n",
      "video 1/1 (79/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.4ms\n",
      "video 1/1 (80/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 222.0ms\n",
      "video 1/1 (81/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 221.5ms\n",
      "video 1/1 (82/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.9ms\n",
      "video 1/1 (83/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 224.0ms\n",
      "video 1/1 (84/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bear, 235.3ms\n",
      "video 1/1 (85/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.5ms\n",
      "video 1/1 (86/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 219.0ms\n",
      "video 1/1 (87/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 218.9ms\n",
      "video 1/1 (88/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 217.9ms\n",
      "video 1/1 (89/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 233.0ms\n",
      "video 1/1 (90/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 1 bear, 230.2ms\n",
      "video 1/1 (91/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 225.5ms\n",
      "video 1/1 (92/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 220.3ms\n",
      "video 1/1 (93/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 232.8ms\n",
      "video 1/1 (94/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 247.0ms\n",
      "video 1/1 (95/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 231.3ms\n",
      "video 1/1 (96/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 229.2ms\n",
      "video 1/1 (97/14713) /Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4: 384x640 (no detections), 223.7ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39m../models/yolov8n-seg.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/adrian/Desktop/cs-projects/sam-2-yolo/notebooks/bbox-idea.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m results \u001b[39m=\u001b[39m trained_model\u001b[39m.\u001b[39;49mpredict(source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4\u001b[39;49m\u001b[39m'\u001b[39;49m, show\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/utils/_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m             \u001b[39m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[1;32m     55\u001b[0m             \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 56\u001b[0m                 response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(request)\n\u001b[1;32m     58\u001b[0m \u001b[39m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m     \u001b[39m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[39m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 240\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/autobackend.py:314\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    311\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:45\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:62\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 82\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     83\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/modules/head.py:94\u001b[0m, in \u001b[0;36mSegment.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m bs \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]  \u001b[39m# batch size\u001b[39;00m\n\u001b[1;32m     93\u001b[0m mc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv4[i](x[i])\u001b[39m.\u001b[39mview(bs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnm, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl)], \u001b[39m2\u001b[39m)  \u001b[39m# mask coefficients\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(\u001b[39mself\u001b[39;49m, x)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m x, mc, p\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/modules/head.py:47\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m shape \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape  \u001b[39m# BCHW\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl):\n\u001b[0;32m---> 47\u001b[0m     x[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2[i](x[i]), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv3[i](x[i])), \u001b[39m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/ultralytics/nn/modules/conv.py:42\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/segment-anything/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = YOLO('../models/yolov8n-seg.pt')\n",
    "results = trained_model.predict(source='../dataset/source/Unique Tigers Collection 8K HDR 60FPS ULTRA HD.mp4', show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
