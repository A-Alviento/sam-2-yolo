{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin\n",
    "Select kernel `segment-anything`\n",
    "\n",
    "Run this cell to import necessary packages and initialise SAM model and mask generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import yaml\n",
    "import glob\n",
    "import pickle\n",
    "from scipy import ndimage\n",
    "\n",
    "from sam2yolo_functions import *\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1' # since mps does not support all the operations, we need to enable fallback to cpu for some operations\n",
    "\n",
    "sam_checkpoint = \"../models/sam_vit_b_01ec64.pth\" # Path to the checkpoint file\n",
    "model_type = 'vit_b' # Model type\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" # Use GPU if available, otherwise use CPU\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint) # Load the model\n",
    "sam.to(device=device) # Move the model to the device\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam) # Create a mask generator\n",
    "mask_predictor = SamPredictor(sam) # Create a mask predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the area of a mask (number of pixels)\n",
    "def get_area(mask):\n",
    "    area = 0\n",
    "    for row in mask:\n",
    "        for col in row:\n",
    "            if col:\n",
    "                area += 1\n",
    "    return area\n",
    "\n",
    "\n",
    "# this function returns the index of the mask with the largest area\n",
    "def get_max_area(masks):\n",
    "    max_area = 0\n",
    "    idx = 0\n",
    "    for i in range(len(masks)):\n",
    "        if(get_area(masks[i]) > max_area):\n",
    "            max_area = get_area(masks[i])\n",
    "            idx = i\n",
    "    return idx\n",
    "\n",
    "\n",
    "# this function draws the mask on the image\n",
    "def overlay_mask_on_image(image, coord):\n",
    "    # Ensure the mask is in 8-bit format\n",
    "    image = cv2.drawContours(image, coord, -1, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "\n",
    "# process the mask to remove the holes in the mask and return the largest region\n",
    "def process_mask(mask):\n",
    "    # Identify each separate region in the mask.\n",
    "    labeled_mask, num_labels = ndimage.label(mask)\n",
    "    \n",
    "    # Count the size of each region.\n",
    "    region_sizes = np.bincount(labeled_mask.flatten())\n",
    "    \n",
    "    # The first region (index 0) is the background, which we don't want to consider.\n",
    "    region_sizes[0] = 0\n",
    "    \n",
    "    # Find the largest region.\n",
    "    largest_region = np.argmax(region_sizes)\n",
    "    \n",
    "    # Create a mask that only includes the largest region.\n",
    "    largest_mask = (labeled_mask == largest_region)\n",
    "    \n",
    "    # Fill in the holes in this region.\n",
    "    filled_mask = ndimage.morphology.binary_fill_holes(largest_mask)\n",
    "    \n",
    "    return filled_mask\n",
    "\n",
    "\n",
    "\n",
    "# extract the coordinates of the segment from SAM and store them in a list\n",
    "def extract_segment(mask):\n",
    "    binary_mask = np.array(mask) # get the segmentation of the mask and convert it to a numpy array\n",
    "    binary_mask = (binary_mask * 255).astype(np.uint8) # convert the mask to a binary mask\n",
    "\n",
    "    binary_mask = (process_mask(binary_mask) * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # find contours from the binary image\n",
    "    polygon_coords = [] # stores the coordinates of the vertices of the polygon\n",
    "    \n",
    "    for contour in contours:\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True) # approximate contour with accuracy proportional to the contour perimeter\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True) # approximate contour with the Douglas-Peucker algorithm\n",
    "\n",
    "        polygon_coords.append(approx) # add the coordinates of the vertices of the polygon to the list\n",
    "    return polygon_coords\n",
    "\n",
    "\n",
    "# this function is to encode the image for rendering by the widget\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded\n",
    "\n",
    "\n",
    "# this function is to encode the image with mask for rendering by the widget, whilst also returning the polygon coordinates of the mask and the original height and width of the image\n",
    "def encode_image_mask(filepath, boxes):\n",
    "    # read in the image file\n",
    "    image = cv2.imread(filepath)\n",
    "    h, w = image.shape[:2]\n",
    "    poly_coords_list = []\n",
    "    for box in boxes:\n",
    "        # convert the bbox to format expected by mask_predictor\n",
    "        box = np.array([\n",
    "            box['x'],\n",
    "            box['y'],\n",
    "            box['x'] + box['width'],\n",
    "            box['y'] + box['height']\n",
    "        ])\n",
    "\n",
    "        mask_predictor.set_image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        # predict the masks\n",
    "        masks, scores, logits = mask_predictor.predict(\n",
    "            box = box,\n",
    "            multimask_output = True\n",
    "        )\n",
    "\n",
    "        # get the index of the mask with the largest area\n",
    "        idx = get_max_area(masks)\n",
    "        mask = masks[idx]\n",
    "\n",
    "        # convert the pixel array format of the mask to a polygon coordinates\n",
    "        polygon_coords = extract_segment(mask)\n",
    "        poly_coords_list.append(polygon_coords)\n",
    "        # overlay the mask on the image\n",
    "        image = overlay_mask_on_image(image, polygon_coords)\n",
    "\n",
    "    # convert the image with mask back to bytes\n",
    "    is_success, im_buf_arr = cv2.imencode(\".jpg\", image)\n",
    "    byte_im = im_buf_arr.tobytes()\n",
    "\n",
    "    # encode to Base64 for rendering on the web\n",
    "    encoded = base64.b64encode(byte_im).decode('utf-8')\n",
    "    \n",
    "    return \"data:image/jpg;base64,\"+encoded, poly_coords_list, h, w\n",
    "\n",
    "\n",
    "\n",
    "# this function initialises the widget\n",
    "def init_widget(path, images, annotations, data, classes, cur_img_idx):\n",
    "    # initialise the bbox widget\n",
    "    w_bbox = BBoxWidget(\n",
    "        image = encode_image(os.path.join(path, images[cur_img_idx])),\n",
    "        classes=classes\n",
    "    )\n",
    "\n",
    "    # initialise the buttons\n",
    "    button_next = widgets.Button(description=\"Next\")\n",
    "    button_prev = widgets.Button(description=\"Previous\")\n",
    "    # combine the buttons and the bbox widget into a container\n",
    "    w_container = widgets.VBox([\n",
    "        w_bbox,\n",
    "        button_prev,\n",
    "        button_next\n",
    "    ])\n",
    "\n",
    "    # function that updates the image when the buttons are clicked so that the next or previous image is shown\n",
    "    def update_image(change):\n",
    "        annotations[images[cur_img_idx]] = w_bbox.bboxes\n",
    "        if change.description == \"Next\":\n",
    "            cur_img_idx = (cur_img_idx + 1) % len(images)\n",
    "        elif change.description == \"Previous\":\n",
    "            cur_img_idx = (cur_img_idx - 1) % len(images)\n",
    "            \n",
    "        # check if annotations[cur_img_idx] exists\n",
    "        if images[cur_img_idx] in annotations:\n",
    "            w_bbox.bboxes = annotations[images[cur_img_idx]]\n",
    "        else:\n",
    "            w_bbox.bboxes = []\n",
    "        w_bbox.image = encode_image(os.path.join(path, images[cur_img_idx]))\n",
    "    # add the update_image function to the buttons\n",
    "    button_next.on_click(update_image)\n",
    "    button_prev.on_click(update_image)\n",
    "\n",
    "    # defines what happens when the submit button is clicked, which is to run SAM with the bounding boxes specified by the user and to display the result and save the result to the data dictionary for conversion to YOLO format later\n",
    "    @w_bbox.on_submit\n",
    "    def submit():\n",
    "        if len(w_bbox.bboxes) > 0:\n",
    "            w_bbox.image, poly_coords_list, h, w = encode_image_mask(os.path.join(path, images[cur_img_idx]), w_bbox.bboxes)\n",
    "            \n",
    "            data[images[cur_img_idx]] = []\n",
    "            \n",
    "            i = 0\n",
    "            for polygon_coords in poly_coords_list:\n",
    "                label_id = [classes.index(w_bbox.bboxes[i]['label'])]\n",
    "                flat_segment_coords = numpy_to_list(polygon_coords)\n",
    "\n",
    "                for j in range(len(flat_segment_coords)): # normalise the coordinates of the segment\n",
    "                    if j%2 == 0:\n",
    "                        flat_segment_coords[j] = flat_segment_coords[j]/w\n",
    "                    else:\n",
    "                        flat_segment_coords[j] = flat_segment_coords[j]/h\n",
    "                \n",
    "                data[images[cur_img_idx]].append(label_id + flat_segment_coords)\n",
    "                i += 1\n",
    "# this is used to extract the frames from a video file and output into specified directory as jpg images\n",
    "def extract_frames(video_path, output_dir, frame_interval=300):\n",
    "    filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"Could not open video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if fps >= 50:\n",
    "        frame_interval *= 2\n",
    "    \n",
    "    frame_index = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = video.read()\n",
    "        if not success: \n",
    "            break\n",
    "\n",
    "        if frame_index % frame_interval == 0:\n",
    "            output_path = os.path.join(output_dir, f\"{filename}_frame_{frame_index}.png\")\n",
    "            cv2.imwrite(output_path, frame)\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "\n",
    "# this is used to split the images into training and validation sets into the dataset folder\n",
    "def split_data(src_directory, out_directory, test_size=0.2):\n",
    "    os.makedirs(out_directory, exist_ok=True) # create the dataset directory\n",
    "\n",
    "    os.makedirs(os.path.join(out_directory, 'train', 'images'), exist_ok=False) # create the train images directory\n",
    "    os.makedirs(os.path.join(out_directory, 'valid', 'images'), exist_ok=False) # create the valid images directory\n",
    "    os.makedirs(os.path.join(out_directory, 'train', 'labels'), exist_ok=False) # create the train labels directory\n",
    "    os.makedirs(os.path.join(out_directory, 'valid', 'labels'), exist_ok=False) # create the valid labels directory\n",
    "\n",
    "    all_files = os.listdir(src_directory) # get all the files in the source directory\n",
    "    train_files, valid_files = train_test_split(all_files, test_size=test_size, random_state=42) # split the files into training and validation sets\n",
    "\n",
    "    # Move files into the train and valid directories\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(src_directory, file_name), os.path.join(out_directory, 'train', 'images', file_name))\n",
    "    for file_name in valid_files:\n",
    "        shutil.copy(os.path.join(src_directory, file_name), os.path.join(out_directory, 'valid', 'images', file_name))\n",
    "\n",
    "\n",
    "# this is used to load the images from the specified directory and output the data in the format required for YOLO training\n",
    "def load_images_from_video(img_path, vid_path, ds_path, frame_interval):\n",
    "    for filename in os.listdir(vid_path):\n",
    "        extract_frames(os.path.join(vid_path, filename), img_path, frame_interval)\n",
    "    \n",
    "    split_data(img_path, ds_path) # splits into train, valid sets and moves into ds_path folder\n",
    "\n",
    "\n",
    "# this is used to format the YOLO data into appropriate txt files for use in YOLO training\n",
    "def output_to_txt(data_dict, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for image_name, content in data_dict.items():\n",
    "        # remove the file extension from the image file name\n",
    "        base_name = os.path.splitext(image_name)[0]\n",
    "        # create the output file name by adding .txt extension\n",
    "        output_file_name = base_name + '.txt'\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        \n",
    "        with open(output_file_path, 'w') as f:\n",
    "            for line in content:\n",
    "                line_str = [str(item) for item in line]  # Convert all items to strings\n",
    "                f.write(','.join(line_str))  # Join all items in the line with ',' as separator\n",
    "                f.write('\\n')  # Write a new line after each line\n",
    "    \n",
    "\n",
    "\n",
    "# this is used to move the data from 3-dataset to dataset for use in YOLO training\n",
    "def move_files(src_img, src_label, dest_img, dest_label):\n",
    "    # Check if destination directories exist, if not, create them\n",
    "    os.makedirs(dest_img, exist_ok=True)\n",
    "    os.makedirs(dest_label, exist_ok=True)\n",
    "\n",
    "    all_images = os.listdir(src_img)\n",
    "    all_labels = os.listdir(src_label)\n",
    "    \n",
    "    for image in all_images:\n",
    "        shutil.copy(os.path.join(src_img, image), os.path.join(dest_img, image))\n",
    "\n",
    "    \n",
    "    for label in all_labels:\n",
    "        shutil.copy(os.path.join(src_label, label), os.path.join(dest_label, label))\n",
    "\n",
    "\n",
    "# this is used to move the videos from 1-source to dataset/sources\n",
    "def move_source_vid(src_vid, dest_vid):\n",
    "    # Check if destination directories exist, if not, create them\n",
    "    os.makedirs(dest_vid, exist_ok=True)\n",
    "\n",
    "    all_videos = os.listdir(src_vid)\n",
    "    \n",
    "    for video in all_videos:\n",
    "        shutil.copy(os.path.join(src_vid, video), os.path.join(dest_vid, video))\n",
    "\n",
    "\n",
    "# clears directory\n",
    "def clear_directory(directory):\n",
    "    # Be careful with this function! It deletes all files and subdirectories in the specified directory\n",
    "    shutil.rmtree(directory)\n",
    "    os.mkdir(directory)\n",
    "\n",
    "\n",
    "# this is used to create the data.yaml (necessary for YOLO training) file in the dataset folder \n",
    "def create_yaml(labels, path, output_path, train_path=\"train/images\", val_path=\"valid/images\", ):\n",
    "    # copies labels with the last two elemens removed\n",
    "    my_dict = {i: labels[i] for i in range(len(labels))}\n",
    "\n",
    "    data = {\n",
    "        'names': my_dict,\n",
    "        'path': path,\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    with open(output_path, 'w') as outfile: # write the data to the yaml file\n",
    "        yaml.dump(data, outfile, default_flow_style=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_path = '../1-source/'\n",
    "img_path = '../2-source-extracted/'\n",
    "ds_path = '../3-dataset/'\n",
    "frame_interval = 300 # specify the frame interval to extract from the video\n",
    "\n",
    "# extract frames from videos in 1-source folder, extract them to 2-source-extracted folder, and split them into train and valid sets in 3-dataset folder\n",
    "load_images_from_video(img_path, vid_path, ds_path, frame_interval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/train/images/' # path to training images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "classes = ['plane', 'airtug'] # list of classes\n",
    "\n",
    "init_widget(path, images, annotations, data, classes, cur_img_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c2cf531cee4780a11032e227e7e2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(BBoxWidget(bboxes=[{'x': 312, 'y': 84, 'width': 89, 'height': 68, 'label': 'plane'}], classes=[â€¦"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/train/labels/') # output the data to txt files for YOLO training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../3-dataset/valid/images/' # path to validation images\n",
    "images = sorted(os.listdir(path))\n",
    "\n",
    "annotations = {} # dictionary with key = image name, value = corresponding bbox\n",
    "data = {} # dictionary with key = image name, value = list of list containing: [label_id, x1, y1, x2, y2, ...]\n",
    "classes = ['plane', 'airtug'] # list of classes\n",
    "\n",
    "init_widget(path, images, annotations, data, classes, cur_img_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_container # display the widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_txt(data, '../3-dataset/valid/labels/') # output the data to txt files for YOLO training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_files('../3-dataset/train/images/', '../3-dataset/train/labels/', '../dataset/train/images/', '../dataset/train/labels/')\n",
    "move_files('../3-dataset/valid/images/', '../3-dataset/valid/labels/', '../dataset/valid/images/', '../dataset/valid/labels/') # moves the data collected in 3-dataset to the actual dataset folder for use in YOLO training\n",
    "move_source_vid('../1-source/', '../dataset/source/') # moves the videos from 1-source to dataset/sources for documentation/backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(input(\"Are you sure you want to clear the source videos, extracted images, and dataset? (y/n): \")) == 'y':\n",
    "    clear_directory('../1-source/')\n",
    "    clear_directory('../2-source-extracted/')\n",
    "    clear_directory('../3-dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abspath_ds = os.path.abspath('../dataset/') # get the absolute path of the dataset folder\n",
    "output_path = '../dataset/data.yaml' # path to the data.yaml file\n",
    "\n",
    "create_yaml(classes, abspath_ds, output_path) # create the data.yaml file in the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segment-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
